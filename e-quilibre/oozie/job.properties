# Fichier de config Oozie (Coordinateur et Workflow)

# environnement de déploiement
contexte= %SITE_DESC%

# paramètre de workflow
appFlux=ENEDIS/CONSOLINE

# @nextline@ nameNode=%SITE_FS%
nameNode=%SITE_FS%
jobTracker=autodetect.HA

# @nextline@ queueName=%SITE_QUEUE%
queueName=%SITE_QUEUE%

# @nextline@ siteRealm=%SITE_REALM%
siteRealm=HADOOP_RE7.EDF.FR

# @nextline@ zookeeperQuorum=%SITE_ZOOKEEPER%
zookeeperQuorum=noeyy5j6.noe.edf.fr:2181,noeyy5jg.noe.edf.fr:2181,noeyy5jj.noe.edf.fr:2181

# uniquement pour les action Hive
# @nextline@ hiveMetastore=%SITE_METASTORE%
hiveMetastore=thrift://noeyy5jg.noe.edf.fr:9083,thrift://noeyy5jj.noe.edf.fr:9083


# application
commonRoot=/user/dco_app/usr
appRoot=/user/dco_app_simm/scripts/ENEDIS/CONSOLINE

#arguments action spark pour la neutralisation des consentements résiliés
appJarRetraitConsentements=Retrait-Consentements-1.63-jar-with-dependencies.jar

inputTables=dco_equilibre:consentements,dco_equilibre:smr_accessibilite
columnfamilies=C,C
dateReprise=null


#arguments action spark pour le traitement HiveToHBase
appJarHiveToHBase=HiveToHBase-1.63-jar-with-dependencies.jar
listR151Functions=fr.edf.dco.hive2hbase.functions.R151PutCfTFunction,fr.edf.dco.hive2hbase.functions.R151PutCfHFunction
listC15Functions=fr.edf.dco.hive2hbase.functions.C15PutCfTFunction,fr.edf.dco.hive2hbase.functions.C15PutCfHFunction
outputTableNameHiveToHBase=dco_equilibre:faits_pdl_linky_bis
requeteHiveR151=select nom_fichier,id_prm,typecalendrier,date_releve,id_calendrier,libelle_calendrier,id_classe_temporelle,libelle_classe_temporelle,rang_cadran,valeur,indice_vraisemblance,pmax,id_affaire,cast(unix_timestamp(concat(date_releve, ' 23:59:59'), 'yyyy-MM-dd HH:mm:ss') as string) as epoch from  erdf.r151_prm_equilibre ORDER BY id_prm, date_releve ASC
#arguments en cas de reprise
repriseFileList='ERDF_R151_17X100A100F0019Z_GRD-F003_4698_00006_M_00001_00001_20170617190536.zip','ERDF_R151_17X100A100F0019Z_GRD-F003_4585_00466_Q_00001_00001_20170617030544.zip'
repriseHiveR151=select nom_fichier,id_prm,typecalendrier,date_releve,id_calendrier,libelle_calendrier,id_classe_temporelle,libelle_classe_temporelle,rang_cadran,valeur,indice_vraisemblance,pmax,id_affaire,cast(unix_timestamp(concat(date_releve, ' 23:59:59'), 'yyyy-MM-dd HH:mm:ss') as string) as epoch from  erdf.r151_prm WHERE nom_fichier in (${repriseFileList}) ORDER BY id_prm, date_releve ASC


#arguments action spark pour le traitement bouchage de trou
appJarBouchageTrou=imputation-linky-0.0.16-jar-with-dependencies.jar
nbMoisMax=1

#arguments action spark pour le traitement HBaseToXML
appJarHBaseToXML=HBaseToXML-1.63-jar-with-dependencies.jar
converterHBaseToXMLName=fr.edf.dco.hbase2xml.converter.R151TokenConverterXML
inputTableNameHBaseToXML=dco_equilibre:faits_pdl_linky_bis,dco_equilibre:consentements
columnfamilyHBaseToXML=H,C
workDirTmp=/user/dco_app_simm/work/ENEDIS/CONSOLINE/tmp
workDirRejets=/user/dco_app_simm/work/ENEDIS/CONSOLINE/rejets



#arguments action shell pour génération des fichiers tar.gz
outputDir=/user/dacc/archive/out/CONSOLINE/2017
outputDirPaas=/user/dacc/staging/out/paas
prefixFileName=EDF_VRN_EDELIA_LINKY_Q_
xmlSize=3000
batchSize=1000


# @nextline@ userName=%INSTALL_USER%
userName=dco_app_simm

# application Kerberos params
krbPrincipal=${userName}@${siteRealm}
krbKeytab=${userName}.keytab
krbAppRoot=/user/${userName}

# librairies
libRoot=/user/dco_app/usr/lib
krbRoot=/user/${userName}
oozie.use.system.libpath=true

# Mails

error_mail=admin-fonc-bmp@edf.fr,eric-externe.rubira@edf.fr,audrey.mayenaquiby@edf.fr,eric-externe.jeannenez@edf.fr,larbi-externe.kennouche@edf.fr,sauguy-externe.sow@edf.fr,dc-tdd-dsi-mco-hadoop@edf.fr
success_mail=admin-fonc-bmp@edf.fr,eric-externe.rubira@edf.fr,audrey.mayenaquiby@edf.fr,eric-externe.jeannenez@edf.fr,sauguy-externe.sow@edf.fr,dc-tdd-dsi-mco-hadoop@edf.fr
cc_mail=dc-verone-mco@edf.fr,larbi-externe.kennouche@edf.fr,sauguy-externe.sow@edf.fr
alert_mail=larbi-externe.kennouche@edf.fr
